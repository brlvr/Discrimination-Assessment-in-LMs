{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brlvr/Discrimination-Assessment-in-LMs/blob/main/00_Tutorial_How_To.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering Workshop"
      ],
      "metadata": {
        "id": "69EzFgqJPXAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the prompt engineering workshop!"
      ],
      "metadata": {
        "id": "ZEKVK6JuPbP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "8AdgkofROn2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***prerequisites***"
      ],
      "metadata": {
        "id": "sZTFe9tROtTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hugging Face account\n",
        "2. Hugging Face API token (Click on your profile image -> Settings -> Access Tokens\n",
        " -> Create new token -> Token type=Read -> give it a name and create!)\n",
        "3. Add the API key in colab secrets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IFjApD3sO4aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get your API key secret"
      ],
      "metadata": {
        "id": "t7MhF055XjZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('API_KEY')"
      ],
      "metadata": {
        "id": "Xzd9sUExXhbu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First steps"
      ],
      "metadata": {
        "id": "vesXjPWEQxGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the `get_completion` function and try to run it with the followed question"
      ],
      "metadata": {
        "id": "4lx8YiMxU4ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Suppress FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "def get_completion(prompt, system_prompt=None, api_key=API_KEY, stream=False, temperature=0.0, max_tokens=300):\n",
        "    # Initialize the Hugging Face inference client with the provided API key\n",
        "    client = InferenceClient(api_key=api_key)\n",
        "\n",
        "    # Setting up the message for the model input\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Add the system prompt if provided\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    try:\n",
        "        if stream:\n",
        "            # Streaming the response from the Hugging Face API\n",
        "            for message in client.chat_completion(\n",
        "                model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Update with the desired model ID\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,  # Add max_tokens parameter\n",
        "                temperature=temperature,  # Add temperature parameter\n",
        "                stream=True\n",
        "            ):\n",
        "                # Printing the streamed response tokens as they arrive\n",
        "                print(message.choices[0].delta.content, end=\"\")\n",
        "        else:\n",
        "            # Non-streaming call, returning the full response at once\n",
        "            response = client.chat_completion(\n",
        "                model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Update with the desired model ID\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,  # Add max_tokens parameter\n",
        "                temperature=temperature,  # Add temperature parameter\n",
        "                stream=False\n",
        "            )\n",
        "            # Extract the full text response\n",
        "            text_content = response['choices'][0]['message']['content']\n",
        "            return text_content\n",
        "\n",
        "    except Exception as err:\n",
        "        # Catch and print any errors\n",
        "        print(f\"An error occurred: {err}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "iO9M0jSkT0yd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT\n",
        "prompt = \"Hello Llamma!\"\n",
        "\n",
        "# Get Llama's response\n",
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDUa-uIyUJXn",
        "outputId": "5a0ee160-4e86-4f39-8d6b-7b69e312ded1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello. How can I assist you today?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}